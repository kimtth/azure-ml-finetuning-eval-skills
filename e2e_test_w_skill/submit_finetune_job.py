"""
Submit a supervised fine-tuning job on Azure ML using TRL SFTTrainer.
Assumes train.jsonl already exists in this folder (generated by generate_customer_support_data.py).
"""
import argparse
import os
from pathlib import Path

from azure.ai.ml import Input, MLClient, Output, command
from azure.ai.ml.entities import Environment
from azure.identity import DefaultAzureCredential


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--subscription-id", default=os.getenv("AZURE_SUBSCRIPTION_ID"))
    parser.add_argument("--resource-group", default=os.getenv("AZURE_RESOURCE_GROUP"))
    parser.add_argument("--workspace", default=os.getenv("AZUREML_WORKSPACE_NAME"))
    parser.add_argument("--compute", required=True)
    parser.add_argument("--data-path", default=str(Path(__file__).parent / "train.jsonl"))
    parser.add_argument(
        "--model-name",
        default="azureml://registries/azureml/models/Phi-3-mini-4k-instruct/versions/1",
        help="Base model that supports fine-tuning; override to Phi-4-mini when available.",
    )
    parser.add_argument("--experiment-name", default="customer-support-sft")
    parser.add_argument("--output-name", default="finetuned-model")
    parser.add_argument("--batch-size", type=int, default=1)
    parser.add_argument("--epochs", type=int, default=1)
    parser.add_argument("--lr", type=float, default=2e-5)
    parser.add_argument("--max-length", type=int, default=512)
    return parser.parse_args()


def main():
    args = parse_args()
    project_dir = Path(__file__).resolve().parent

    env = Environment(
        name="customer-support-sft-env",
        image="mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu22.04",
        conda_file=str(project_dir / "environment" / "conda.yml"),
    )

    ml_client = MLClient(
        DefaultAzureCredential(),
        args.subscription_id,
        args.resource_group,
        args.workspace,
    )

    job = command(
        code=str(project_dir / "src"),
        command=(
            "python train_sft.py "
            "--model_name ${{inputs.model_name}} "
            "--train_data ${{inputs.train_data}} "
            "--output_dir ${{outputs.model_dir}} "
            "--batch_size ${{inputs.batch_size}} "
            "--epochs ${{inputs.epochs}} "
            "--lr ${{inputs.lr}} "
            "--max_length ${{inputs.max_length}}"
        ),
        inputs={
            "model_name": args.model_name,
            "train_data": Input(type="uri_file", path=args.data_path, mode="ro_mount"),
            "batch_size": args.batch_size,
            "epochs": args.epochs,
            "lr": args.lr,
            "max_length": args.max_length,
        },
        outputs={"model_dir": Output(type="uri_folder", mode="rw_mount", path=args.output_name)},
        environment=env,
        compute=args.compute,
        display_name="customer-support-sft",
        experiment_name=args.experiment_name,
    )

    submitted_job = ml_client.jobs.create_or_update(job)
    print(f"Job: {submitted_job.name}")
    studio_service = submitted_job.services.get("Studio") if submitted_job.services else None
    studio_url = studio_service.endpoint if studio_service else None
    print(f"Studio: {studio_url or 'Open the job in Azure ML studio to monitor'}")


if __name__ == "__main__":
    main()
